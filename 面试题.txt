----------阶段1----------

1. 解释有监督学习，无监督学习和半监督学习的概念
答：（1）有监督学习是对已经标注的数据进行训练，训练过程中每一个输入数据都与一个对应的目标标签关联，模型任务是学习输入到输出的映射关系；（2）无监督学习在没有标签的数据上进行训练，模型要自行发现数据的结构或模式；（3）半监督学习是使用一小部分标注的数据和大量未标注的数据进行学习，先用标注的数据训练一个初步模型，随后对未标注的数据进行预测，选择高置信度的预测结果作为伪标签加到训练集中再进行训练。

2. 什么是过拟合和欠拟合，如何避免？
答：（1）过拟合：在训练数据表现得很好，新的、未见过的测试数据表现很差，过拟合模型在训练数据上学习了过多噪声和细节，导致其无法泛化到新的数据上。（2）欠拟合：模型在训练数据和测试数据上的表现都很差，本质是无法捕捉数据的基本模式，可能的原因是模型过于简单或训练不足。（3）如何避免过拟合：在loss上加入惩罚项（weight_decay），加入dropout，用focal loss，或者label smoothing，以及使用不同的label weights，K折交叉验证、early stopping（验证集的loss过大的时候使用）。（4）如何避免欠拟合：增加模型复杂度，延长训练时间。

3. 写出对数损失函数的公式
答：L = - \left[ y \log(p) + (1 - y) \log(1 - p) \right]
L = - \sum_{i=1}^{C} y_i \log(p_i)
y是one hot向量，p是对于每一个类别的预测概率向量。

4. 如何解决类别不平衡问题？
答：（1）类别加权，可以给少数的样本更大的权重，比如L = - \sum_{i=1}^{C} \alpha_i \cdot y_i \log(p_i)；（2）使用focal loss，公式为FL(p_t) = - \alpha_t (1 - p_t)^\gamma \log(p_t)。

5. 回归模型和分类模型常用损失函数有哪些？各有什么优缺点？
答：回归模型：（1）均方误差损失，MSE，易于计算，在误差较小的情况下，对预测有较好的梯度表现，缺点是容易对异常值（outliers）敏感，放大其误差的影响（鼓励模型去完美拟合噪声样本的趋势）；（2）均绝对误差损失，MAE，MAE相较于MSE对异常值的鲁棒性更强，不因为单个大误差点而产生大的变化，梯度不平滑，导致不稳定的优化过程；
分类模型：（1）交叉熵损失CE，最常用的分类损失函数，对概率输出敏感，能提高模型分类能力，缺点是对不平衡数据敏感，受标签影响大，导致不稳定；（2）focal loss，针对类别不平衡问题，比CE的处理效果要更好

6. 如何选择模型中的超参数？有什么方法？
7. 模型的“泛化”能力是指？如何提升模型泛化能力？
8. 什么是梯度消失和爆炸
9. 什么是 dropout?
10. 激活函数有什么作用？
11. 什么是最大池化？它的作用是什么？
12. 为什么归一化能够提高求解最优解的速度？
13. 在参数初始化时，为什么不能全零初始化？
14. RELU函数有什么优缺点？
15. word2vec提出了负采样的策略，它的原理是什么，解决了什么样的问题？
16. BERT的结构和原理是什么?
17. 如果让你实现一个命名实体识别任务，你会怎么设计?

----------阶段2----------
1. 谈谈ChatGPT的优缺点
答：优点如下--（1）强自然语言理解和生成能力，能处理多种对话场景，解决技术、日常交流、创作等问题；（2）多语言支持；（3）知识覆盖面广。缺点如下--（1）缺少深度推理能力，无法判断9.9和9.11谁大谁小的问题；（2）生成不确定性，会有幻觉问题；（3）处理长对话上下文丢失，多轮对话中，容易忘记早期提问或回答的问题。

2. 请简述下Transformer基本流程
3. 为什么基于Transformer的架构需要多头注意力机制？
4. 编码器，解码器，编解码LLM模型之间的区别是什么？
5. 你能解释在语言模型中强化学习的概念吗？它如何应用于ChatGPT？
6. 在GPT模型中，什么是温度系数？
7. 什么是旋转位置编码（ROPE）？
8. 为什么现在的大模型大多是decoder-only的架构？
9. ChatGPT的训练步骤有哪些？
10. 为什么transformers需要位置编码？
11. 为什么对于ChatGPT而言，提示工程很重要？
12. 如何缓解 LLMs 复读机问题？

