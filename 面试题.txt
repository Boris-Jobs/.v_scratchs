----------阶段1----------

1. 解释有监督学习，无监督学习和半监督学习的概念
2. 什么是过拟合和欠拟合，如何避免？
3. 写出对数损失函数的公式
4. 如何解决类别不平衡问题？
5. 回归模型和分类模型常用损失函数有哪些？各有什么优缺点？
6. 如何选择模型中的超参数？有什么方法？
7. 模型的“泛化”能力是指？如何提升模型泛化能力？
8. 什么是梯度消失和爆炸
9. 什么是 dropout?
10. 激活函数有什么作用？
11. 什么是最大池化？它的作用是什么？
12. 为什么归一化能够提高求解最优解的速度？
13. 在参数初始化时，为什么不能全零初始化？
14. RELU函数有什么优缺点？
15. word2vec提出了负采样的策略，它的原理是什么，解决了什么样的问题？
16. BERT的结构和原理是什么?
17. 如果让你实现一个命名实体识别任务，你会怎么设计?

----------阶段2----------
1. 谈谈ChatGPT的优缺点
2. 请简述下Transformer基本流程
3. 为什么基于Transformer的架构需要多头注意力机制？
4. 编码器，解码器，编解码LLM模型之间的区别是什么？
5. 你能解释在语言模型中强化学习的概念吗？它如何应用于ChatGPT？
6. 在GPT模型中，什么是温度系数？
7. 什么是旋转位置编码（ROPE）？
8. 为什么现在的大模型大多是decoder-only的架构？
9. ChatGPT的训练步骤有哪些？
10. 为什么transformers需要位置编码？
11. 为什么对于ChatGPT而言，提示工程很重要？
12. 如何缓解 LLMs 复读机问题？

