----------阶段0-1----------

1. 解释有监督学习，无监督学习和半监督学习的概念
答：（1）有监督学习是对已经标注的数据进行训练，训练过程中每一个输入数据都与一个对应的目标标签关联，模型任务是学习输入到输出的映射关系；（2）无监督学习在没有标签的数据上进行训练，模型要自行发现数据的结构或模式；（3）半监督学习是使用一小部分标注的数据和大量未标注的数据进行学习，先用标注的数据训练一个初步模型，随后对未标注的数据进行预测，选择高置信度的预测结果作为伪标签加到训练集中再进行训练。

2. 什么是过拟合和欠拟合，如何避免？
答：（1）过拟合：在训练数据表现得很好，新的、未见过的测试数据表现很差，过拟合模型在训练数据上学习了过多噪声和细节，导致其无法泛化到新的数据上。（2）欠拟合：模型在训练数据和测试数据上的表现都很差，本质是无法捕捉数据的基本模式，可能的原因是模型过于简单或训练不足。（3）如何避免过拟合：在loss上加入惩罚项（weight_decay），加入dropout，用focal loss，或者label smoothing，以及使用不同的label weights，K折交叉验证、early stopping（验证集的loss过大的时候使用）。（4）如何避免欠拟合：增加模型复杂度，延长训练时间。

3. 写出对数损失函数的公式
答：L = - \left[ y \log(p) + (1 - y) \log(1 - p) \right]
L = - \sum_{i=1}^{C} y_i \log(p_i)
y是one hot向量，p是对于每一个类别的预测概率向量。

4. 如何解决类别不平衡问题？
答：（1）类别加权，可以给少数的样本更大的权重，比如L = - \sum_{i=1}^{C} \alpha_i \cdot y_i \log(p_i)；（2）使用focal loss，公式为FL(p_t) = - \alpha_t (1 - p_t)^\gamma \log(p_t)。

5. 回归模型和分类模型常用损失函数有哪些？各有什么优缺点？
答：回归模型：（1）均方误差损失，MSE，易于计算，在误差较小的情况下，对预测有较好的梯度表现，缺点是容易对异常值（outliers）敏感，放大其误差的影响（鼓励模型去完美拟合噪声样本的趋势）；（2）均绝对误差损失，MAE，MAE相较于MSE对异常值的鲁棒性更强，不因为单个大误差点而产生大的变化，梯度不平滑，导致不稳定的优化过程；
分类模型：（1）交叉熵损失CE，最常用的分类损失函数，对概率输出敏感，能提高模型分类能力，缺点是对不平衡数据敏感，受标签影响大，导致不稳定；（2）focal loss，针对类别不平衡问题，比CE的处理效果要更好

6. 如何选择模型中的超参数？有什么方法？
答：（1）学习率一般在1e-5到1e-1之间，会从1e-4开始选择，一般会使用学习率调度器，余弦退火，指数衰减，还有step decay，warmup等等调整学习率；（2）epoch设置50-100，会通过交叉验证、学习曲线来看是否要early stopping或者调整epoch；（3）batch size一般会选择32或者64来做默认值，同时看GPU内存的大小是否会对batch size有限制；（4）正则化，L2正则化的lambda一般设置到0.01-0.001之间，dropout比率一般设置为0.2-0.5之间

7. 模型的“泛化”能力是指？如何提升模型泛化能力？
答：泛化指的是在未见过的新数据上的表现能力，好的泛化能力意味着从训练集学习到了有用的模式，能够处理未见过的具有不同分布的测试数据；提升泛化能力：（1）数据增强，通过对数据变换，比如旋转平移缩放等，生成新的训练样本，帮模型学习更多特征，（2）正则化，降低过拟合风险，提高泛化能力，（3）交叉验证，数据分部分轮流进行训练和验证，得到泛化能力最强的模型

8. 什么是梯度消失和爆炸
答：梯度消失就是梯度在多层神经网络传递的时候，梯度变得越来越小，接近于0，使得训练缓慢甚至停止；梯度爆炸指的是反向传播过程中梯度值变得异常的大，导致权重更新幅度过大，比如某些层的梯度大于1，会导致大的梯度逐层累计，导致梯度爆炸

9. 什么是 dropout?
答：训练过程，dropout会随机地将一些神经元的输出值设置为0，每一个神经元都有p的概率被丢弃，1-p的概率进行保留，同时未被丢弃的神经元会被放大，其方式为保证输出的节点值期望值与未丢弃时一样，测试时不使用dropout

10. 激活函数有什么作用？
加入非线性因素使神经网络可以学习任何复杂的模式。

11. 什么是最大池化？它的作用是什么？
答：降低空间维度、保留重要特征，比如2*2的最大池化对28*28的图像进行下采样，就可以将维度变成14*14，降低计算量，同时最大池化可以保留图像里的重要信息，边缘、角点等等

12. 为什么归一化能够提高求解最优解的速度？
答：比如输入数据特征范围差异很大，有的特征在0-1之间，有的在1000-10000之间，这样会导致大的特征主导梯度计算，这样会导致权重更新不平衡，使得收敛过程变缓慢，归一化可以让每个特征方向进行均匀更新，保证数值稳定性，减少收敛时间

13. 在参数初始化时，为什么不能全零初始化？
答：举个例子，比如y=w1x，y2=w2y，L=y_-y2，这里要是w1和w2都为0初始化，会导致反向传播的过程w1和w2的梯度均为0，无法更新，无法进行学习

14. RELU函数有什么优缺点？
答：（1）优点：计算简单、同时有效减缓梯度消失的问题、收敛速度快，（2）缺点：负值输出的神经元会变成死神经元、不适合处理所有数据分布的情况

15. word2vec提出了负采样的策略，它的原理是什么，解决了什么样的问题？
答：The cat sat on the mat，先选定一个中心词，sat，（sat，the），（sat，cat）就是正样本对，（sat，dog），（sat，apple）就是负样本对，正样本对通常是中心词的上下文词汇，负样本对通常是从词表的高频词汇里随机选择的，（1）Word2Vec标准训练方式需要计算整个词表里面的条件概率，意味着需要计算所有词与目标词向量的内积，通过负采样就能大大减少计算量。（2）同时负采样引入负样本，强化词与词之间的区分度，有助于学习更好的词向量表示，提高模型泛化能力。

16. BERT的结构和原理是什么?
答：（1）BERT的输入：token embeddings+segment embeddings+positional embeddings，（2）BERT由多个Transformer的Encoder组成，每一层都有自注意力机制和前馈神经网络。
预训练目标：（1）BERT的与训练目标有Masked Language Model，输入的句子会随机掩蔽掉一些词，15%的词汇会被掩蔽，同时通过双向自注意力机制来考虑上下文信息，（2）Next Sentence Prediction，这个任务会给两个句子，一个句子对连续，一个句子对非连续，让模型判断句子是否连续。

17. 如果让你实现一个命名实体识别任务，你会怎么设计?
答：（1）问题定义：给定一段文本，模型要识别所有的实体，对其进行标注，（2）数据准备：
John    B-PER
lives    O
in        O
New     B-LOC
York     I-LOC
.          O
数据集如CoNLL-2003。（3）输入：Token IDs、Attention Mask、Token Type IDs、Labels（用BIO来对每一个token进行标注），（4）选择模型，如BERT，在BERT的顶层添加分类层，如果含有15个类别标签，那么就使得输出维度为15，（5）随后训练并评估。

----------阶段1----------
1. 谈谈ChatGPT的优缺点
答：优点如下--（1）强自然语言理解和生成能力，能处理多种对话场景，解决技术、日常交流、创作等问题；（2）多语言支持；（3）知识覆盖面广。缺点如下--（1）缺少深度推理能力，无法判断9.9和9.11谁大谁小的问题；（2）生成不确定性，会有幻觉问题；（3）处理长对话上下文丢失，多轮对话中，容易忘记早期提问或回答的问题。

2. 请简述下Transformer基本流程
答：（1）输入，词嵌入+位置编码，（2）自注意力机制，计算Query和Key的相似度，再用相似度得到对Value的得分，同时Transformer的自注意力机制为多头注意力机制，可以不同角度学习到序列的关系，每一个头学习到一个d_model/h大小的向量，最后做concatenate，（3）LN（Embed+Atten（Embed））→LN（（ReLU（xW+b）W1+b1）+x）是Encoder的过程，（4）解码器由三部分，1. Masked Multi-Head Self-Attention，2. Encoder-Decoder Attention以及3. Feed-Forward Neural Network，解码器的输出会通过线性层和Softmax激活函数，通常会把输出向量映射到词汇表大小的维度，预测每个单词的概率大小。

3. 为什么基于Transformer的架构需要多头注意力机制？
答：1. 多个角度捕捉序列的语法信息，增强表征能力；2. 提高计算的效率

4. 编码器，解码器，编解码LLM模型之间的区别是什么？
答：（1）Encoder主要是把输入数据转化为隐空间表示，embedding；（2）Decoder主要是把embedding表示成最终的输出，比如文本生成、翻译、摘要；（3）编解码LLM就是Encoder和Decoder通过注意力机制进行信息交换，Decoder不仅依赖Encoder生成的隐空间表示，还利用自回归生成输出。

5. 你能解释在语言模型中强化学习的概念吗？它如何应用于ChatGPT？
答：（1）RL的策略就是agent通过试错来最大化积累奖励，与监督学习不同的是，RL不依赖于已标注的训练数据，而依赖agent和env的交互过程，（2）RLHF可以用于优化ChatGPT，GPT模型先通过最大化似然估计MLE获得语言生成能力，随后生成回答让人类标注员进行打分，评分作为奖励信号，评分数据训练一个Reward  Model，随后通过Proximal Policy Optimization对模型进行进一步训练。

6. 在GPT模型中，什么是温度系数？
答：T主要用于最后一层的softmax的logits的分母，T大于1会让生成更有随机性，因为会让概率分布更加平缓，T=1就不做任何变化，T小于1会让生成文本更加确定和一致，概率分布更加尖锐。

7. 什么是旋转位置编码（ROPE）？
答：先对文本的token进行嵌入生成Q和K向量，随后用旋转操作将Q和K的嵌入向量奇偶元素两两一组做旋转变换作为对应位置的token向量的新值，旋转位置编码最本质的好处是可以给出token的相对位置信息，避免绝对位置编码带来信息丧失，而且对于动态长度输入也很有效。

8. 为什么现在的大模型大多是decoder-only的架构？
答：自回归模型，主要做生成，不需要单独的编码阶段，根据上下文预测下一个最有可能的单词，decoder-only模型利用masked self-attention来考虑先前单词的序列关系做生成，适合这种任务。计算效率上，有KV-cache的方法提高效率，而且训练成本可以降低，因为decoder-only的泛化能力强。

9. ChatGPT的训练步骤有哪些？
答：（1）数据准备：爬数据，清洗数据，标准化格式，（2）自回归训练，给定上下文预测下一个词，（3）监督微调SFT+RLHF，（4）最后进行测试评估，BLEU score、ROUGE score、Perplexity、Accuracy

10. 为什么transformers需要位置编码？
答：（1）自注意力机制训练的时候，序列的token是考虑全局信息的而没有依赖于顺序信息，所以需要靠位置编码来给出顺序信息，（2）帮助模型理解语法结构。

11. 为什么对于ChatGPT而言，提示工程很重要？
答：（1）ChatGPT不是为特定任务单独训练的模型，所以提示清晰且有指导性的prompt可以通过上下文来优化生成，（2）减少无效的推理

12. 如何缓解 LLMs 复读机问题？
答：（1）调整温度系数，让模型的输出更加多样化，（2）控制最大生成长度，（3）RLHF

----------------------------------------------------------------
手撕transformer、位置编码公式、评估指标公式

